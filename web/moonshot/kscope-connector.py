import kscope
import logging
import os
from typing import Any

from moonshot.src.connectors.connector import Connector, perform_retry
from moonshot.src.connectors_endpoints.connector_endpoint_arguments import (
    ConnectorEndpointArguments,
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KscopeConnector(Connector):
    def __init__(self, ep_arguments: ConnectorEndpointArguments):

        logger.info(f"Initializing KscopeConnector...")

        token = ep_arguments.token
        jwt_file = os.path.join(os.path.expanduser("~"), ".kaleidoscope.jwt")

        # Check if the token file already exists and is non-empty
        if os.path.isfile(jwt_file) and os.path.getsize(jwt_file) > 0:
            logger.info("JWT token already exists")
        else:
            logger.info("JWT token does not exist, writing it now") 
            try:
                with open(jwt_file, "w") as file:
                    file.write(token)
            except Exception as err:
                raise Exception(f"Unable to write the token specified in your model endpoint: {err}")

        # Initialize super class
        super().__init__(ep_arguments)

        # Setup kscope client
        kscope_uri = ep_arguments.uri.replace("http://", "")
        self._client = kscope.Client(kscope_uri.split(":")[0], kscope_uri.split(":")[1])

        # Set the model. Defaults to llama3-8b unless specified otherwise in optional_params
        self.model = self.optional_params.get("model", "llama3-8b")

        # Check if the model is supported. If not, raise an exception
        if self.model not in self._client.models:
            raise Exception(f"The {self.model} model is not supported in Kaleidoscope.")

        # Check if the model is activate. If not, raise an expception.
        is_model_active = False
        for instance in self._client.model_instances:
            if instance["name"] == self.model and instance["state"] == "ACTIVE":
                is_model_active = True

        if not is_model_active:
            raise Exception(f"The {self.model} model is not active and loaded in Kaleidoscope. Please contact an administrator to load it.")

    @Connector.rate_limited
    @perform_retry
    async def get_response(self, prompt: str) -> str:
        """
        Asynchronously sends a prompt to the Kscope API and returns the generated response.

        This method constructs a request with the given prompt, optionally prepended and appended with
        predefined strings, and sends it to the Kscope API. If a system prompt is set, it is included in the
        request. The method then awaits the response from the API, processes it, and returns the resulting message
        content as a string.

        Args:
            prompt (str): The input prompt to send to the Kscope API.

        Returns:
            str: The text response generated by the Kscope model.
        """
        kscope_model = self._client.load_model(self.model)
        response = kscope_model.generate(prompt)
        result_text = response.generation['sequences'][0]

        return result_text

    async def _process_response(self, response: Any) -> str:
        """
        Process the response from Kscope's API and return the message content as a string.

        This method processes the response received from Kscope's API call, specifically targeting
        the chat completion response structure. It extracts the message content from the first choice
        provided in the response, which is expected to contain the relevant information or answer.

        Args:
            response (Any): The response object received from an Kscope API call. It is expected to
            follow the structure of Kscope's chat completion response.

        Returns:
            str: A string containing the message content from the first choice in the response. This
            content represents the AI-generated text based on the input prompt.
        """
        return response.choices[0].message.content
